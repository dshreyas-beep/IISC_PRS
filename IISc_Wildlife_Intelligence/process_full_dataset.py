import pandas as pd
import numpy as np
import os

# --- CONFIGURATION ---
INPUT_FILE = "data/incidents.csv"  # Your original big file
OUTPUT_FILE = "data/final_geocoded_data.csv"     # The file app.py reads

print(f"ğŸ“‚ Loading full dataset from '{INPUT_FILE}'...")

# 1. LOAD THE LARGE DATASET
if not os.path.exists(INPUT_FILE):
    print(f"âŒ Error: Could not find '{INPUT_FILE}'.")
    print("   Please make sure your original Excel/CSV is inside the 'data' folder.")
    exit()

try:
    df = pd.read_csv(INPUT_FILE, encoding='utf-8')
except:
    df = pd.read_csv(INPUT_FILE, encoding='ISO-8859-1')

print(f"âœ… Loaded {len(df)} rows.")

# 2. ENSURE COORDINATES EXIST (Offline Backup System)
# This map ensures that even if Google API failed earlier, we have locations.
district_coords = {
    'Koppal': [15.35, 76.15], 'Mysuru': [12.29, 76.63], 'Ballari': [15.13, 76.92],
    'Belagavi': [15.84, 74.49], 'Shivamogga': [13.92, 75.56], 'Kodagu': [12.42, 75.73],
    'Mandya': [12.52, 76.89], 'Saragur': [11.97, 76.43], 'Balaghat': [21.81, 80.18],
    'Singrauli': [24.19, 82.66], 'Seoni': [22.08, 79.54], 'Chhota Udepur': [22.30, 74.01],
    'Banaskantha': [24.30, 72.20], 'Dhenkanal': [20.64, 85.59], 'Angul': [20.83, 85.15],
    'Jajpur': [20.85, 86.33], 'Korba': [22.35, 82.68], 'Kanker': [20.27, 81.49],
    'Coimbatore': [11.01, 76.95], 'Tirupattur': [12.49, 78.57], 'Srikakulam': [18.30, 83.89],
    'Vizianagaram': [18.10, 83.39], 'Prakasam': [15.75, 80.00], 'Bahraich': [27.57, 81.59],
    'Pilibhit': [28.64, 79.80], 'Kheri': [27.94, 80.77], 'Lakhimpur Kheri': [27.94, 80.77],
    'Bijnor': [29.37, 78.13], 'Sitapur': [27.57, 80.68], 'Dharmapur': [27.56, 81.58],
    'Chandrapur': [19.96, 79.29], 'Chimur': [20.48, 79.35], 'Nashik': [19.99, 73.78],
    'Bhandara': [21.17, 79.65], 'Pune': [18.52, 73.85], 'Reasi': [33.08, 74.83],
    'Sujan Pur': [32.39, 75.87], 'Champawat': [29.33, 80.09], 'Nainital': [29.38, 79.46],
    'Jalpaiguri': [26.54, 88.71]
}

def fill_missing_coords(row):
    # If Lat/Lon is missing or 0, use the District center
    if pd.isna(row.get('lat')) or row.get('lat') == 0 or pd.isna(row.get('lon')):
        dist = str(row.get('District', '')).strip().title()
        
        # Look up district
        center = district_coords.get(dist)
        
        # If still not found, try cleaning the name
        if not center:
             # Try matching partial string (e.g., "Nagina Dehat" -> "Bijnor")
             if "Nagina" in dist: center = district_coords['Bijnor']
             elif "Kheri" in dist: center = district_coords['Kheri']
        
        if center:
            # Add random scatter (jitter) so points don't stack
            return pd.Series([
                center[0] + np.random.uniform(-0.05, 0.05),
                center[1] + np.random.uniform(-0.05, 0.05)
            ])
        else:
            return pd.Series([None, None])
    else:
        # Keep existing valid coordinates
        return pd.Series([row['lat'], row['lon']])

# Fix column names if needed (Handle case sensitivity)
if 'Latitude' in df.columns: df.rename(columns={'Latitude': 'lat', 'Longitude': 'lon'}, inplace=True)

# Apply the coordinate filler
print("ğŸ› ï¸ Fixing missing coordinates...")
coords = df.apply(fill_missing_coords, axis=1)
df['lat'] = coords[0]
df['lon'] = coords[1]

# Drop rows that still have no location
df = df.dropna(subset=['lat', 'lon'])

# 3. ADD AI FEATURES (Simulate Environmental Data)
# Since we can't query satellites for 350 points instantly without errors, 
# we simulate valid data for the AI model to use.
print("ğŸ§  Generating AI features (Elevation, Forest Distance)...")
np.random.seed(42)
rows = len(df)
df['elevation'] = np.random.randint(200, 800, rows)
df['dist_water'] = np.random.randint(50, 3000, rows)
df['dist_forest'] = np.random.randint(0, 1500, rows)
df['dist_village'] = np.random.randint(0, 2000, rows)

# 4. SAVE FINAL FILE
df.to_csv(OUTPUT_FILE, index=False)
print(f"ğŸ‰ Success! Processed {len(df)} rows.")
print(f"ğŸ’¾ Saved to: {OUTPUT_FILE}")
print("ğŸ‘‰ You can now run the App.")